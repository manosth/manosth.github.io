<html>
<head>
  <title>Columbia Interview Question</title>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Raleway:400,900" rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" type="text/css" href="../css/posts.css">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
  </script>
  <link rel="stylesheet"
     href="../css/styles/atom-one-dark.css">
  <script src="../js/highlight.pack.js"></script>
</head>

<body>
<div class="main_container">
<div class="content_container">
  <div style="padding-top: 5px">
      <p style="float:right; margin-top:20px; font-size:300%;">
         <a class="text-link-head" href="https://manosth.github.io/blog"><i class="fas fa-arrow-left"></i></a>
      </p>
  <div class="title_text">
  <div>Columbia Interview Question</div>
  </div>
  <hr>
  </div>

<div style="clear:both;"></div>
<h3>Introduction</h3>
I once heard two people at my university discussing a math problem. They claimed that a friend of theirs was asked, during a Skype interview for admission to Columbia's Computer Science PhD program, the following question:
<br>
<br>
<center>
    <p style="font-size: 95%; background: rgba(0, 0, 255, 0.25); border-radius: 5px;">
    <i>"Suppose the following experiment: we draw random samples, one at a time, from a uniform distribution. The experiment ends when we draw a sample that was greater from the last. What is the expected time $n$ that the experiment will run for?"</i>
    </p>
</center>
Now, this is a very interesting question. On the surface, it looks very simple and easy. For simplicity, let's assume that the probability distribution is $\mathcal{U}(0, 1)$. Essentially this means that at each step we are picking a random number in the range $[0, 1]$. The mean of this distribution is $\mu = 0.5$. So, if we choose a sample at random, it is most likely to be close to $0.5$.
<br>
<br>
    <center>
        <img src="../images/figure0.svg" width="80%" alt="First sample at 0.5"/><br>
        <br>
        Figure 1: Expected placing for the first sample based on the mean $\mu$.
    </center>
<br>
So what about the second sample we draw? Well, since it is uniformly random, a sample has a $50\%$ chance of being <i>greater</i> than the mean $\mu$, and a $50\%$ chance of being <i>smaller</i> than the mean $\mu$.
<br>
<br>
    <center>
        <img src="../images/figure1.svg" width="80%" alt="Second sample unknown"/><br>
        <br>
        Figure 2: Two possible placings for the second sample.
    </center>
<br>
It seems pretty possible that the experiment could end at $n = 2$, right? And even if it didn't end at the second sample, it seems very likely it would end, on average, at the third sample. This is because, sice $x_2$ is smaller than $x_1$, the values for $x_3$ that would allow for the experiment to continue are even <i>fewer</i>.
<br>
<br>
    <center>
        <img src="../images/figure2.svg" width="80%" alt="Third sample small"/><br>
        <br>
        Figure 3: Possible placings for the third sample.
    </center>
<br>
<br>
To test our theory, we can write a small Python program that will perform a few such experiments and see the results:
<pre style="background: none; border: none;"><code class="python">import random
t = 1000000

winning = []
for idx in range(t):
    prev = random.random()
    nxt = random.random()
    counter = 2
    while (prev >= nxt):
        prev = nxt
        nxt = random.random()
        counter += 1
    winning.append(counter)

print "Average: " + str(sum(winning) / float(t))
</code></pre>

In the above code, we run $t = 1000000$ iterations of the experiment and stored in a list the value $n$ at which each experiment ended. By running the above code, we get an average value of $2.719059$. Our intuition was correct! Many of the experiments will end at the second draw, and the ones that go on after the third draw are <i>very</i> limited. So why is this question interesting? Because deriving the result formally, and providing an exact answer, is a lot more intricate.
<br>
<br>
<h3>Formalising the problem</h3>
Let us model the experiment using a random variable $X$. The random variable $X$ denotes the probability of the experiment ending after $n$ draws. Since we need to draw at least two samples for the experiment to end, it follows that $n \geq 2$. The answer to the question is essentially the expected value of $X$ &nbsp;
$$
E[X] = \sum_{n=2}^{\infty}n \cdot P[X = n]
$$
Let's examine what exactly is the value of $P[X = n]$. To do that, let's denote the samples as $x_1, ..., x_n$. Then, the sample $x_2$ needs to be smaller than $x_1$, $x_3$ needs to be smaller than $x_2$, and so on, with $x_n$ being <i>greater</i> than $x_{n-1}$. Formally
$$
\begin{align}
    P[X = n] &= P[x_n > x_{n-1}, x_{n-1} < x_{n-2}, x_{n-2} < x_{n-3}, ..., x_{2} < x_{1}]\\
         &= P[x_n > x_{n-1}, x_{n-1} < x_{n-2} < x_{n-3} < ... < x_{2} < x_{1}]\\
\end{align}
$$
since we need the first $n-1$ samples to be in a descending order, and the $n-$th sample to be greater than the last. Still, this probability seems incomputable. We are going to change that. Using the <a class="text-link" href="https://en.wikipedia.org/wiki/Conditional_probability" target="_blank">Law of Conditional Probability</a> we can break the above probability into two parts
$$
\begin{align}
    P[X = n] &= P[x_n > x_{n-1}, x_{n-1} < x_{n-2} < x_{n-3} < ... < x_{2} < x_{1}]\\
         &= P[x_n > x_{n-1} | x_{n-1} < x_{n-2} < x_{n-3} < ... < x_{2} < x_{1}] \cdot P[x_{n-1} < x_{n-2} < x_{n-3} < ... < x_{2} < x_{1}]\\
\end{align}
$$
Now we have two probabilities to compute; however, they are significantly easier to compute:
<ul>
    <li> Let's consider the second one first. What does $P[x_{n-1} < x_{n-2} < ... < x_{1}]$ really mean? In essence, it defines an ordering of $n-1$ items. Suppose we have $n-1$ labeled balls and we want to put them in a line. <a class="text-link" href="https://en.wikipedia.org/wiki/Combinatorics" target="_blank">Combinatorics</a> tells us that the different possible orderings of $n-1$ items are $(n-1)!$. To understand this we can think of it as trying to place an item at each point of the order. When choosing the item for the first position, we have $n$ possible choices. Then, when we are choosing the item for the second position, since one item was placed in the first position, we have $n-1$ possible choices. So, if we have to order $5$ items all the possible combinations are $5 + 4 + 3 + 2 + 1 = 5!$.
    <br>
    <br>
        <center>
            <img src="../images/figure3.svg" width="80%" alt="Combinatorics"/><br>
            <br>
            Figure 3: Ordering 5 labeled balls.
        </center>
    <br>
    Since all the $x_i$ are samples from a uniform distribution, we can argue that each of these orderings are equally likely. We want to enforce a specific ordering, namely the order $x_{n-1} < x_{n-2} < x_{n-3} < ... < x_{2} < x_{1}$. Therefore, since we want only one out of $(n-1)!$ possible orderings, the probability is $P[x_{n-1} < x_{n-2} < ... < x_{1}] = \frac{1}{(n-1)!}$.<br><br>
    </li>
    <li>Consider a given ordering of balls $x_{\kappa_1} < x_{\kappa_2} < ... x_{\kappa_{n-1}}$, where $\kappa_i \in 1, 2, ..., n-1$. Then observing a new sample can go into one of $n$ possible positions.
    <br>
    <br>
        <center>
            <img src="../images/figure4.svg" width="80%" alt="New sample position"/><br>
            <br>
            Figure 4: Possible position for the new sample.
        </center>
    <br>
    Again, because the distribution is uniform, each of these positions are equally likely<sup><a class="text-link" href="#footnote1">1</a></sup>. So at what place does the probabilty $P[x_n > x_{n-1} | x_{n-1} < x_{n-2} < ... < x_{1}]$ wants the new sample placed? The only condition is that $x_n$ needs to be larger than $x_{n-1}$. But $x_{n-1}$ is the leftmost ball in the ordering. So, any position <i>but</i> the leftmost one will satisfy this constraint. Since, out of the $n$ possible positions, the $n-1$ satisfy the constaint, it follows that $P[x_n > x_{n-1} | x_{n-1} < x_{n-2} < ... < x_{1}] = \frac{n-1}{n}$.</li>
</ul>
Combining these two results, we can now compute the original probability
$$
P[X = n] = \frac{n-1}{n} \cdot \frac{1}{(n-1)!} = \frac{1}{n} \cdot \frac{1}{(n-2)!}
$$
and then the expected value of the random variable $X$ is
$$
E[X] = \sum_{n=2}^{\infty}n \cdot P[X = n] = \sum_{n=2}^{\infty}n \cdot \frac{1}{n} \cdot \frac{1}{(n-2)!} = \sum_{n=2}^{\infty} \frac{1}{(n-2)!}
$$
Let us perform a change of variable, so that the above expression is easier to recognise. Let $k = n-2$, then the above becomes
$$
E[X] = \sum_{k=0}^{\infty} \frac{1}{k!} = e
$$
where $e$ is the <a class="text-link" href="https://en.wikipedia.org/wiki/E_(mathematical_constant)" target="_blank">mathematical constant</a>.
<br>
<br>
<h3>Discussion</h3>
I find this problem very interesting for a few reasons:
<ul>
    <li>First of all, it is very simple to state, uderstand, and explain. It is also very simple to reason; everyone can understand that, because of its nature, it is highly unlikely that the <i>average</i> running time of the experiment is greater than, say, $10$.</li>
    <li>While reasoning about this problem seems to be rather easy, the formal analysis is significantly more convoluted. I think this is interesting because it allows people to think; we often dismiss certain statements because they seem obvious, and we might be correct. However, many obvious statements have rather obfuscated explanations.</li>
    <li>The mathematical constant $e$ arises at the very end. It is very frequent for students to question teachers about how the mathematical constants are used in practice; this is a good, and unexpected, example.</li>
</ul>
<hr>
<p><sup><span style="color:#306EFF;" id="footnote1">1</span></sup><span style="font-size:85%;">: &nbsp;This is not trivial to understand or to prove. Intuitively, since we have taken $n-1$ uniformly distributed samples, the probabilistic assumption is that they will be uniformly distributed. Then another sample taken from that uniform distribution is equally likely to fall in each of the bins created by the other samples. This can be formally proven using mathematical induction, but it is cumbersome.</span></p>
</div>
<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>
